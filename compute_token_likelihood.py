import torch
import torch.nn as nn
import math
import torch.nn.functional as F
from transformers import LEDConfig, LEDForConditionalGeneration, LEDTokenizer, logging
import numpy as np
import argparse
from tqdm import tqdm
import re
from tokenizers import Tokenizer
from torch.nn.parallel import DistributedDataParallel as DDP
import torch.distributed as dist
from multiprocessing import Manager
import torch.multiprocessing as mp
from panGPT import setup, cleanup
import random
from torch.utils.data import DataLoader, DistributedSampler
from panGPT import GenomeDataset, load_dataset
from collections import defaultdict
import pickle
import sys
from statistics import mean
from compute_SHAP import 

logging.set_verbosity_error()

def insert_tensor(tensor, value_tensor, insert_pos):
    num_insertions = value_tensor.numel()
    # shift values down
    #print(tensor[0, insert_pos], file=sys.stderr)
    #print(tensor, file=sys.stderr)
    tensor[0, insert_pos + num_insertions:] = tensor[0, insert_pos:-num_insertions].clone()
    
    # Insert new values
    tensor[0, insert_pos:insert_pos + num_insertions] = value_tensor
    #print(tensor[0, insert_pos], file=sys.stderr)
    #print(tensor, file=sys.stderr)
    #return tensor

def insert_list(lst, i, new_elements):
    return lst[:i] + new_elements + lst[i:]

def generate_gene_id(unsplit_id):
    name = unsplit_id.split("SAM")[1].split("_")[0].split(".")[0]
    split_gene = unsplit_id.split("_")
    gene_id = split_gene[-1]
    contig_id = split_gene[-2][-5:]

    gene_name = name + "_" + contig_id + "_" + gene_id

    return gene_name

def parse_args():
    """
    Parse command-line arguments.

    This function parses the command-line arguments provided by the user and returns
    a Namespace object containing the parsed arguments.
    """
    parser = argparse.ArgumentParser(description="Token prediction with a Transformer or Reformer model.")
    parser.add_argument("--model_path", type=str, required=True, help="Path to the model checkpoint file.")
    parser.add_argument("--tokenizer_path", type=str, required=True, help="Path to the tokenizer file.")
    parser.add_argument("--prompt_file", type=str, required=True, help="Path to the text file containing the prompt.")
    parser.add_argument("--gene_list", type=str, required=True, help="Path to the text file containing a list of representative gene IDs to calculate probabilities for, with ID in first column e.g. output of Bakta _full.tsv file.")
    parser.add_argument("--reps_dict", type=str, required=True, help="Path to the representative -> token dictionary .pkl file generated by tokenise_clusters.py.")
    parser.add_argument("--embed_dim", type=int, default=256, help="Embedding dimension.")
    parser.add_argument("--num_heads", type=int, default=8, help="Number of attention heads.")
    parser.add_argument("--num_layers", type=int, default=8, help="Number of transformer layers.")
    parser.add_argument("--max_seq_length", type=int, default=16384, help="Maximum sequence length.")
    parser.add_argument("--model_dropout_rate", type=float, default=0.2, help="Dropout rate for the model")
    parser.add_argument("--batch_size", type=int, default=16, help="Maximum batch size for simulation. Default = 16")
    parser.add_argument("--device", type=str, default=None, help="Device to run the model on (e.g., 'cpu' or 'cuda').")
    parser.add_argument("--attention_window", type=int, default=512, help="Attention window size in the Longformer model (default: 512)")
    parser.add_argument("--max_input_len", type=int, default=None, help="Maximum length of input sequence. No limit if not set.")
    parser.add_argument("--min_input_len", type=int, default=None, help="Minimum length of input sequence. No limit if not set.")
    parser.add_argument("--outpref", type=str, default="simulated_genomes", help="Output prefix for simulated genomes. Default = 'simulated_genomes'")
    parser.add_argument("--DDP", action="store_true", default=False, help="Multiple GPUs used via DDP during training.")
    parser.add_argument("--encoder_only", default=False, action="store_true", help="Prompt using encoder input only.")
    parser.add_argument("--randomise", default=False, action="store_true", help="Randomise sequence for upon input.")
    parser.add_argument("--global_contig_breaks", default=False, action="store_true", help="Attend globally to contig breaks. Default is local only.")
    parser.add_argument("--parse_gene_id", default=False, action="store_true", help="Augment gene IDs to old format. Default = False.")
    parser.add_argument("--max-SHAP", default=False, action="store_true", help="Calculate SHAP values for position with highest pseudolikelihood")
    parser.add_argument("--port", default="12356", type=str, help="GPU port for DDP. Default=12356")
    parser.add_argument("--seed", default=42, type=int, help="Seed for randomisation")

    args = parser.parse_args()

    # Ensure max_seq_length is greater than or equal to attention_window
    args.max_seq_length = max(args.max_seq_length, args.attention_window)
    # Round down max_seq_length to the nearest multiple of attention_window
    args.max_seq_length = (args.max_seq_length // args.attention_window) * args.attention_window

    return args

def pad_input(input, max_length, pad_token_id, labels=False):

    len_masked = len(input)
    # only pad if necessary
    if len_masked >= max_length:
        pass
    else:
        if labels == False:
            input.extend([pad_token_id] * (max_length - len_masked))
        else:
            input.extend([-100] * (max_length - len_masked))

    return input

def load_model(embed_dim, num_heads, num_layers, max_seq_length, device, vocab_size, attention_window, model_dropout_rate):

    BARTlongformer_config = LEDConfig(
        vocab_size=vocab_size,
        d_model=embed_dim,
        encoder_layers=num_layers,
        decoder_layers=num_layers,
        encoder_attention_heads=num_heads,
        decoder_attention_heads=num_heads,
        decoder_ffn_dim=4 * embed_dim,
        encoder_ffn_dim=4 * embed_dim,
        max_encoder_position_embeddings=max_seq_length,
        max_decoder_position_embeddings=max_seq_length,
        dropout=model_dropout_rate,
        attention_window = attention_window
        )
    model = LEDForConditionalGeneration(BARTlongformer_config)
    model.config.use_cache = True
    return model

def get_pseudolikelihood(outputs, i, j, token_id):
    log_pseudo_likelihood = 0
    for pos in range(i, j):
        logits = outputs[0, pos]
        #print(token_id, file=sys.stderr)
        #print(logits, file=sys.stderr)
        # Get the probability of the original token
        token_prob = torch.softmax(logits, dim=-1)[token_id].item()
        #print(token_prob, file=sys.stderr)
        #print(torch.log(torch.tensor(token_prob)).to("cpu").item(), file=sys.stderr)

        # Add the log probability to the total log-pseudo-likelihood
        log_pseudo_likelihood += torch.log(torch.tensor(token_prob)).to("cpu").item()
    
    #print(log_pseudo_likelihood, file=sys.stderr)
    return log_pseudo_likelihood

def calculate_gene_pseudolikelihood(model, tokenizer, loader, device, max_seq_length, encoder_only, token_query_list, max_SHAP, args):
    model.eval()
    mask_token = tokenizer.encode("<mask>").ids[0]
    pad_token = tokenizer.encode("<pad>").ids[0]

    for_token_query_tensor = torch.tensor([token_id_for for gene_id, token_id_for, token_id_rev in token_query_list]).reshape(len(token_query_list),1).to(device)
    rev_token_query_tensor = torch.tensor([token_id_rev for gene_id, token_id_for, token_id_rev in token_query_list]).reshape(len(token_query_list),1).to(device)
    attention_mask_tensor = torch.ones(len(token_query_list), dtype=torch.long).reshape(len(token_query_list),1).to(device)
    global_attention_mask_tensor = torch.zeros(len(token_query_list), dtype=torch.long).reshape(len(token_query_list),1).to(device)
    addition_length = len(token_query_list)
    gene_id = "_".join([gene_id for gene_id, token_id_for, token_id_rev in token_query_list])

    total_gene_list = []
    average_gene_dict = defaultdict(list)
    with torch.no_grad():
        # repeat for number of sequences required. Means each sequences is masked in different ways
        for decoder_input, encoder_input, labels, decoder_attention_mask, encoder_attention_mask, global_attention_mask in tqdm(loader, desc="Genome iteration", unit="batch"):  # Correctly unpack the tuples returned by the DataLoader

            total_len = encoder_input.size(1)

            genome_gene_dict = defaultdict(list)

            for i in range(0, total_len, max_seq_length):
                # print("max_seq_length exceeded: {}".format(total_len > max_seq_length))

                #print(labels)
                if encoder_only:
                    batch_encoder_input, batch_encoder_attention_mask, batch_global_attention_mask = encoder_input[:, i:i + max_seq_length].to(device), encoder_attention_mask[:, i:i + max_seq_length].to(device), global_attention_mask[:, i:i + max_seq_length].to(device) # Move data to the appropriate device

                    # iterate over whole sequence for masking
                    token_count = max_seq_length
                    for j in range(len(batch_encoder_input[0])):
                        if batch_encoder_input[0, j] == pad_token:
                            token_count = j
                            break
                    
                    # extend tensors
                    masked_batch_encoder_attention_mask = batch_encoder_attention_mask.clone()
                    masked_batch_global_attention_mask = batch_global_attention_mask.clone()

                    #masked_batch_encoder_attention_mask = 
                    #print("masked_batch_encoder_attention_mask", file=sys.stderr)
                    insert_tensor(masked_batch_encoder_attention_mask, attention_mask_tensor, 0)
                    #masked_batch_global_attention_mask = 
                    #print("masked_batch_global_attention_mask", file=sys.stderr)
                    insert_tensor(masked_batch_global_attention_mask, global_attention_mask_tensor, 0)
                    masked_batch_encoder_attention_mask.to(device)
                    masked_batch_global_attention_mask.to(device)

                    for j in range(0, token_count):
                        #print(f"token: {j}", file=sys.stderr)
                        masked_encoder_input = batch_encoder_input.clone()
                        if masked_encoder_input[0, j] == pad_token:
                            break
                        
                        # forward direction
                        #masked_encoder_input = 
                        #print("masked_encoder_input for", file=sys.stderr)
                        insert_tensor(masked_encoder_input, for_token_query_tensor, j)
                        masked_encoder_input[0, j:j + addition_length] = mask_token
                        masked_encoder_input = masked_encoder_input.to(device)
                    
                        outputs = model(input_ids=masked_encoder_input, attention_mask=masked_batch_encoder_attention_mask, global_attention_mask=masked_batch_global_attention_mask).logits  # Generate predictions
                        
                        log_pseudo_likelihood_gene_for = 0
                        
                        for token_idx, token in enumerate(for_token_query_tensor):
                            log_pseudo_likelihood_gene_for += get_pseudolikelihood(outputs, j, j + token_idx + 1, token.item())
                        
                        # reverse direction
                        masked_encoder_input = batch_encoder_input.clone()
                        #masked_encoder_input = 
                        #print("masked_encoder_input rev", file=sys.stderr)
                        insert_tensor(masked_encoder_input, rev_token_query_tensor, j)
                        masked_encoder_input[0, j:j + addition_length] = mask_token
                        masked_encoder_input = masked_encoder_input.to(device)

                        outputs = model(input_ids=masked_encoder_input, attention_mask=masked_batch_encoder_attention_mask, global_attention_mask=masked_batch_global_attention_mask).logits  # Generate predictions
                        log_pseudo_likelihood_gene_rev = 0
                        
                        for token_idx, token in enumerate(rev_token_query_tensor):
                            log_pseudo_likelihood_gene_for += get_pseudolikelihood(outputs, j, j + token_idx + 1, token.item())
                        
                        # take average of forward and reverse
                        log_pseudo_likelihood_gene = (log_pseudo_likelihood_gene_for + log_pseudo_likelihood_gene_rev) / 2
                            
                        genome_gene_dict[gene_id].append(log_pseudo_likelihood_gene)
                            
                        # Free GPU memory
                        del masked_encoder_input

                    # Free GPU memory
                    del batch_encoder_input
                    del batch_encoder_attention_mask
                    del batch_global_attention_mask
                    del masked_batch_decoder_attention_mask
                    del masked_batch_global_attention_mask
                    del masked_batch_encoder_attention_mask

                else:
                    batch_decoder_input, batch_encoder_input, batch_decoder_attention_mask, batch_encoder_attention_mask, batch_global_attention_mask = decoder_input[:, i:i + max_seq_length].to(device), encoder_input[:, i:i + max_seq_length].to(device), decoder_attention_mask[:, i:i + max_seq_length].to(device), encoder_attention_mask[:, i:i + max_seq_length].to(device), global_attention_mask[:, i:i + max_seq_length].to(device) # Move data to the appropriate device
                    
                    # iterate over whole sequence for masking
                    token_count = max_seq_length
                    for j in range(len(batch_encoder_input[0])):
                        if batch_encoder_input[0, j] == pad_token:
                            token_count = j
                            break

                    # extend tensors
                    masked_batch_encoder_attention_mask = batch_encoder_attention_mask.clone()
                    masked_batch_global_attention_mask = batch_global_attention_mask.clone()
                    masked_batch_decoder_attention_mask = batch_decoder_attention_mask.clone()   

                    #masked_batch_encoder_attention_mask = 
                    #print("masked_batch_encoder_attention_mask", file=sys.stderr)
                    insert_tensor(masked_batch_encoder_attention_mask, attention_mask_tensor, 0)
                    #masked_batch_global_attention_mask = 
                    #print("masked_batch_global_attention_mask", file=sys.stderr)
                    insert_tensor(masked_batch_global_attention_mask, global_attention_mask_tensor, 0)
                    #masked_batch_decoder_attention_mask = 
                    #print("masked_batch_decoder_attention_mask", file=sys.stderr)
                    insert_tensor(masked_batch_decoder_attention_mask, attention_mask_tensor, 0)
                    masked_batch_encoder_attention_mask.to(device)
                    masked_batch_global_attention_mask.to(device)
                    masked_batch_decoder_attention_mask.to(device)

                    for j in range(0, token_count):
                        #print(f"token: {j}", file=sys.stderr)
                        masked_encoder_input = batch_encoder_input.clone()
                        if masked_encoder_input[0, j] == pad_token:
                            break
                        
                        masked_batch_decoder_input = batch_decoder_input.clone()              
                        
                        # forward direction
                        #masked_encoder_input = 
                        #print("masked_encoder_input for", file=sys.stderr)
                        insert_tensor(masked_encoder_input, for_token_query_tensor, j)
                        #masked_batch_decoder_input = 
                        #print("masked_batch_decoder_input for", file=sys.stderr)
                        insert_tensor(masked_batch_decoder_input, for_token_query_tensor, j + 1)
                        masked_encoder_input[0, j:j + addition_length] = mask_token
                        masked_batch_decoder_input[0, j + 1:(j + 1) + addition_length] = mask_token
                        masked_encoder_input = masked_encoder_input.to(device)
                        masked_batch_decoder_input = masked_batch_decoder_input.to(device)
                    
                        outputs = model(input_ids=masked_encoder_input, attention_mask=masked_batch_encoder_attention_mask, decoder_input_ids=masked_batch_decoder_input, decoder_attention_mask=masked_batch_decoder_attention_mask, global_attention_mask=masked_batch_global_attention_mask).logits  # Generate predictions

                        log_pseudo_likelihood_gene_for = 0
                        
                        for token_idx, token in enumerate(for_token_query_tensor):
                            log_pseudo_likelihood_gene_for += get_pseudolikelihood(outputs, j, j + token_idx + 1, token.item())
                        
                        # reverse direction
                        masked_encoder_input = batch_encoder_input.clone()
                        #masked_encoder_input = 
                        #print("masked_encoder_input rev", file=sys.stderr)
                        insert_tensor(masked_encoder_input, rev_token_query_tensor, j)
                        masked_batch_decoder_input = batch_decoder_input.clone()
                        #masked_batch_decoder_input = 
                        #print("masked_batch_decoder_input rev", file=sys.stderr)
                        insert_tensor(masked_batch_decoder_input, rev_token_query_tensor, j + 1)
                        masked_encoder_input[0, j:j + addition_length] = mask_token
                        masked_batch_decoder_input[0, j + 1:(j + 1) + addition_length] = mask_token
                        masked_encoder_input = masked_encoder_input.to(device)
                        masked_batch_decoder_input = masked_batch_decoder_input.to(device)

                        outputs = model(input_ids=masked_encoder_input, attention_mask=masked_batch_encoder_attention_mask, decoder_input_ids=masked_batch_decoder_input, decoder_attention_mask=masked_batch_decoder_attention_mask, global_attention_mask=masked_batch_global_attention_mask).logits  # Generate predictions

                        log_pseudo_likelihood_gene_rev = 0
                        
                        for token_idx, token in enumerate(rev_token_query_tensor):
                            log_pseudo_likelihood_gene_rev += get_pseudolikelihood(outputs, j, j + token_idx + 1, token.item())
                        
                        # take average of forward and reverse
                        log_pseudo_likelihood_gene = (log_pseudo_likelihood_gene_for + log_pseudo_likelihood_gene_rev) / 2
                        #print(f"log_pseudo_likelihood_gene: {log_pseudo_likelihood_gene}", file=sys.stderr)    
                        
                        genome_gene_dict[gene_id].append(log_pseudo_likelihood_gene)
                            
                        # Free GPU memory
                        del masked_encoder_input
                        del masked_batch_decoder_input
                    
                    # Free GPU memory
                    del batch_encoder_input
                    del batch_encoder_attention_mask
                    del batch_decoder_input
                    del batch_decoder_attention_mask
                    del batch_global_attention_mask
                    del masked_batch_decoder_attention_mask
                    del masked_batch_global_attention_mask
                    del masked_batch_encoder_attention_mask
            
            # calculate whole genome gene likelihoods
            total_gene_list.append(genome_gene_dict)
            
            for gene_id, token_id_for, token_id_rev in token_query_list:
                average_likelihood_gene = mean(genome_gene_dict[gene_id])
                average_gene_dict[gene_id].append(average_likelihood_gene)

            # get SHAP values for highest SHAP position, only works for single loci currently
            if max_SHAP:
                prompt_list = []
                for genome_idx in range(0, len(loader)):
                    genome = loader.getitem(genome_idx)
                    
                    # get position of highest pseuodolikelihood value
                    for gene_id, genome_gene_list in genome_gene_dict:
                        max_value = max(genome_gene_list)
                        max_idx = None
                        for idx, value in enumerate(genome_gene_list):
                            if value == m:
                                max_idx = idx
                                break
                    
                    # insert into specific location
                    split_genome = genome.split(" ")

                    for_token_query_list = [token_id_for for gene_id, token_id_for, token_id_rev in token_query_list]
                    rev_token_query_list = [token_id_rev for gene_id, token_id_for, token_id_rev in token_query_list]
                    
                    for_split_genome = insert_list(split_genome, max_idx, for_token_query_list)
                    rev_split_genome = insert_list(split_genome, max_idx, rev_token_query_list)
                    prompt_list.append(for_split_genome)
                    prompt_list.append(rev_split_genome)

                target_token = token_query_list[0][0]
                calculate_SHAP(model, tokenizer, prompt_list, device, max_seq_length, encoder_only, target_token, args.outpref, args.seed, args)

    return total_gene_list, average_gene_dict

def read_prompt_file(file_path):
    prompt_list = []
    with open(file_path, 'r') as file:
        for line in file:
            prompt_list.append(line.strip())
    return prompt_list

def split_prompts(prompts, world_size):
    # Split prompts into approximately equal chunks for each GPU
    chunk_size = len(prompts) // world_size
    return [prompts[i * chunk_size:(i + 1) * chunk_size] for i in range(world_size)]

def query_model(rank, model_path, world_size, args, BARTlongformer_config, tokenizer, prompt_list, token_query_list, DDP_active, encoder_only, return_list, gene_list):
    if DDP_active:
        setup(rank, world_size, args.port)
        #prompt_list = prompt_list[rank]
        sampler = DistributedSampler(prompt_list, num_replicas=world_size, rank=rank, shuffle=False)
        num_workers = 0
        pin_memory = False
        shuffle = False
    else:
        sampler = None
        pin_memory = True
        shuffle = False
        num_workers=1
    
    dataset = GenomeDataset(prompt_list, tokenizer, args.max_seq_length, 0, args.global_contig_breaks, False)
    dataset.attention_window = args.attention_window
    loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=pin_memory, sampler=sampler)
    
    model = LEDForConditionalGeneration(BARTlongformer_config)
    device = rank
    model = model.to(device)
    if DDP_active:
        model = DDP(model, device_ids=[rank], find_unused_parameters=True)

    map_location = None
    if DDP_active:
        map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}
        dist.barrier()
    
    if map_location != None:
        checkpoint = torch.load(model_path, map_location=map_location)
    else:
        checkpoint = torch.load(model_path)
    model.load_state_dict(checkpoint["model_state_dict"])

    master_process = rank == 0

    total_gene_list, average_gene_dict = calculate_gene_pseudolikelihood(model, tokenizer, loader, device, args.max_seq_length, encoder_only, token_query_list, args.max_SHAP, prompt_list)
    
    return_list.extend(total_gene_list)
    gene_list.append(average_gene_dict)
        
def main():
    args = parse_args()

    tokenizer = Tokenizer.from_file(args.tokenizer_path)
    vocab_size = tokenizer.get_vocab_size()

    args.max_seq_length = max(args.max_seq_length, args.attention_window)
    # Round down max_seq_length to the nearest multiple of attention_window
    args.max_seq_length = (args.max_seq_length // args.attention_window) * args.attention_window
    device = args.device

    DDP_active = args.DDP

    BARTlongformer_config = LEDConfig(
        vocab_size=vocab_size,
        d_model=args.embed_dim,
        encoder_layers=args.num_layers,
        decoder_layers=args.num_layers,
        encoder_attention_heads=args.num_heads,
        decoder_attention_heads=args.num_heads,
        decoder_ffn_dim=4 * args.embed_dim,
        encoder_ffn_dim=4 * args.embed_dim,
        max_encoder_position_embeddings=args.max_seq_length,
        max_decoder_position_embeddings=args.max_seq_length,
        dropout=args.model_dropout_rate,
        attention_window = args.attention_window
        )
    
    world_size = torch.cuda.device_count()
    if DDP_active:
        if world_size > 0:
            # Use DDP but just one GPU
            if device != None:
                device = torch.device("cuda:{}".format(device))
                world_size = 1
            else:
                device = torch.device("cuda") # Run on a GPU if one is available
            print("{} GPU(s) available, using cuda".format(world_size))
        else:
            print("GPU not available, using cpu.")
            device = torch.device("cpu")
    else:
        if world_size > 0 and device != "cpu":
            device = torch.device("cuda:{}".format(device))
        else:
            device = torch.device("cpu")

    prompt_list = load_dataset(args.prompt_file)

    # randomise
    if args.randomise:
        prompt_list = [genome.split() for genome in prompt_list]
        for genome in prompt_list:
            random.shuffle(genome)
        prompt_list = [" ".join(genome) for genome in prompt_list]

    # remove sequences that are too long or short
    if args.max_input_len != None:
        # len_list = [len(genome.split()) for genome in prompt_list]
        # print(len_list)
        prompt_list = [genome for genome in prompt_list if len(genome.split()) <= args.max_input_len]

    if args.min_input_len != None:
        # len_list = [len(genome.split()) for genome in prompt_list]
        # print(len_list)
        prompt_list = [genome for genome in prompt_list if len(genome.split()) >= args.min_input_len]

    with open(args.reps_dict, 'rb') as handle:
        token_to_rep, reps_dict = pickle.load(handle)
        # invert reps_dict
        #reps_dict = dict((v, k) for k, v in reps_dict.items())
        #del reps_dict

    # generate tokenized list of genes
    token_query_list = []
    with open(args.gene_list, "r") as f:
        for line in f:
            split_line_ori = line.rstrip().split()[0]

            if args.parse_gene_id:
                split_line = generate_gene_id(split_line_ori)
            else:
                split_line = split_line_ori
            try:
                gene_token = reps_dict[split_line]
                token_tuple = (tokenizer.encode(str(gene_token)).ids[0], tokenizer.encode(str(-1 * gene_token)).ids[0])
                decoded_token_tuple = (tokenizer.decode([token_tuple[0]], skip_special_tokens=False), tokenizer.decode([token_tuple[1]], skip_special_tokens=False))
                # don't add unknown tokens
                if decoded_token_tuple[0] == "<unk>" and decoded_token_tuple[1] == "<unk>":
                    print(f"{split_line_ori} not in tokenizer")
                else:
                    token_query_list.append((split_line_ori, token_tuple[0], token_tuple[1]))
            except KeyError:
                print(f"{split_line_ori} not in reps_dict")
    
    if len(token_query_list) == 0:
        print("No valid tokens found.")
        sys.exit(0)

    with open(args.outpref + "_genes_to_tokens.txt", "w") as f:
        f.write("Gene_ID\tToken_for\tToken_rev\n")
        for gene_id, token_for, token_rev in token_query_list:
            f.write(f"{gene_id}\t{str(token_for)}\t{str(token_rev)}\n")

    return_list = []
    gene_list = None
    if DDP_active:
        #prompt_list = split_prompts(prompt_list, world_size)
        with Manager() as manager:
            mp_list = manager.list()
            gene_mp_list = manager.list()
            mp.spawn(query_model,
                    args=(args.model_path, world_size, args, BARTlongformer_config, tokenizer, prompt_list, token_query_list, DDP_active, args.encoder_only, mp_list, gene_mp_list),
                    nprocs=world_size,
                    join=True)
            return_list = list(mp_list)
            gene_list = list(gene_mp_list)
    else:
        query_model(device, args.model_path, 1, args, BARTlongformer_config, tokenizer, prompt_list, token_query_list, DDP_active, args.encoder_only, return_list, gene_list)
    
    # unpack gene_list, create dictionary where 
    genome_dict = defaultdict(list)
    for entry in gene_list:
        for gene_id, genome_list in entry.items():
            for genome_idx, likelihood in enumerate(genome_list):
                genome_dict[genome_idx].append(likelihood)
    
    # decode and write output
    with open(args.outpref + "_genome_pseudolikelihood.txt", "w") as f:
        f.write("Genome_ID\t" + "\t".join([x[0] for x in token_query_list]) + "\n")
        for genome_idx, value_list in genome_dict.items():
            f.write(str(genome_idx) + "\t")
            for value in value_list:
                f.write(f"{str(value)}\t")
            f.write("\n")

    with open(args.outpref + "_locus_pseudolikelihood.txt", "w") as f:
        f.write("Genome_Index\tGene_ID\t" + "\t".join([str(x) for x in range(0, args.max_seq_length)]) + "\n")
        for genome_idx, likelihood_dict in enumerate(return_list):
            for gene_id, likelihood_list in likelihood_dict.items():
                
                # add zeros to fill list up
                while len(likelihood_list) < args.max_seq_length:
                    likelihood_list.append(0.0)

                f.write(str(genome_idx) + "\t" + gene_id + "\t" + "\t".join([str(x) for x in likelihood_list]) + "\n")

    if DDP_active:
        cleanup()

if __name__ == "__main__":
    main()
